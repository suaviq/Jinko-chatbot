{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "use_builtins = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_to_file):\n",
    "    file = open(path_to_file,'r').read()\n",
    "    qna_list = [f.split('\\t') for f in file.split('\\n')]\n",
    "    x = [x[0] for x in qna_list]\n",
    "    y = [y[1] for y in qna_list]\n",
    " \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_data('D:/chatbot_project/archive/dialogs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A TF.DATA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(x)\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_SIZE = int(0.8 * BUFFER_SIZE/BATCH_SIZE)\n",
    "TEST_SIZE = int(0.2 * BUFFER_SIZE/BATCH_SIZE)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_data = dataset.take(TRAIN_SIZE)\n",
    "test_data = dataset.skip(TRAIN_SIZE)\n",
    "# val_data = test_data.skip(VAL_SIZE)\n",
    "# test_data = test_data.take(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example_input_batch, example_target_batch in dataset.take(1):\n",
    "#   print(example_input_batch[:5])\n",
    "#   print()\n",
    "#   print(example_target_batch[:5])\n",
    "#   break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT PROCESSING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "# print(example_text.numpy())\n",
    "# print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  text = tf.strings.strip(text)\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_text.numpy().decode())\n",
    "# print(tf_lower_and_split_punct(example_text).numpy().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x000001CC6DD7C160>\n"
     ]
    }
   ],
   "source": [
    "input_text_processor.adapt(x)\n",
    "print(input_text_processor)\n",
    "\n",
    "# # Here are the first 10 words from the vocabulary:\n",
    "# input_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "\n",
    "output_text_processor.adapt(y)\n",
    "# output_text_processor.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_tokens = input_text_processor(example_input_batch)\n",
    "# example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "# tokens = input_vocab[example_tokens[0].numpy()]\n",
    "# ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1, 2, 1)\n",
    "# plt.pcolormesh(example_tokens)\n",
    "# plt.title('Token IDs')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.pcolormesh(example_tokens != 0)\n",
    "# plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining constants for the model \n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.enc_units = enc_units\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "\n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # The GRU RNN layer processes those vectors sequentially.\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   # Return the sequence and state\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, tokens, state=None):\n",
    "    # 2. The embedding layer looks up the embedding for each token.\n",
    "    vectors = self.embedding(tokens)\n",
    "    # 3. The GRU processes the embedding sequence.\n",
    "    #    output shape: (batch, s, enc_units)\n",
    "    #    state shape: (batch, enc_units)\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "    # 4. Returns the new sequence and its state.\n",
    "    return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the input text to tokens.\n",
    "# example_tokens = input_text_processor(example_input_batch)\n",
    "\n",
    "# Encode the input sequence.\n",
    "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)\n",
    "# example_enc_output, example_enc_state = encoder(example_tokens)\n",
    "\n",
    "# print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
    "# print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
    "# print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
    "# print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    # score(ht, hs) = \n",
    "    # Luong's multiplicative style: ht.transpose*W*hs\n",
    "    # Bahdanau's additive style: Va.transpose* tanh(W1*ht+W2*hs)\n",
    "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "  def call(self, query, value, mask):\n",
    "    # W1*ht\n",
    "    w1_query = self.W1(query)\n",
    "\n",
    "    # W2*hs\n",
    "    w2_key = self.W2(value)\n",
    "\n",
    "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "    value_mask = mask\n",
    "\n",
    "    context_vector, attention_weights = self.attention(\n",
    "        inputs = [w1_query, value, w2_key],\n",
    "        mask=[query_mask, value_mask],\n",
    "        return_attention_scores = True,\n",
    "    )\n",
    "\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (example_tokens != 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Later, the decoder will generate this attention query\n",
    "# example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
    "\n",
    "# # Attend to the encoded tokens\n",
    "\n",
    "# context_vector, attention_weights = attention_layer(\n",
    "#     query=example_attention_query,\n",
    "#     value=example_enc_output,\n",
    "#     mask=(example_tokens != 0))\n",
    "\n",
    "# print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
    "# print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplot(1, 2, 1)\n",
    "# plt.pcolormesh(attention_weights[:, 0, :])\n",
    "# plt.title('Attention weights')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.pcolormesh(example_tokens != 0)\n",
    "# plt.title('Mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_weights.shape\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "  new_tokens: Any\n",
    "  enc_output: Any\n",
    "  mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "  logits: Any\n",
    "  attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_slice = attention_weights[0, 0].numpy()\n",
    "# attention_slice = attention_slice[attention_slice != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "  def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "    vectors = self.embedding(inputs.new_tokens)\n",
    "    rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "    context_vector, attention_weights = self.attention(query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "    attention_vector = self.Wc(context_and_rnn_output)\n",
    "    logits = self.fc(attention_vector)\n",
    "\n",
    "    return DecoderOutput(logits, attention_weights), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder.call = call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
    "                  embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the target sequence, and collect the \"[START]\" tokens\n",
    "# example_output_tokens = output_text_processor(example_target_batch)\n",
    "\n",
    "# start_index = output_text_processor.get_vocabulary().index('[START]')\n",
    "# first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the decoder\n",
    "# dec_result, dec_state = decoder(\n",
    "#     inputs = DecoderInput(new_tokens=first_token,\n",
    "#                           enc_output=example_enc_output,\n",
    "#                           mask=(example_tokens != 0)),\n",
    "#     state = example_enc_state\n",
    "# )\n",
    "\n",
    "# print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
    "# print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = np.array(output_text_processor.get_vocabulary())\n",
    "# first_word = vocab[sampled_token.numpy()]\n",
    "# first_word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOSS FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss'\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss = self.loss(y_true, y_pred)\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    loss *= mask\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING STEP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainChatbot(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units,\n",
    "               input_text_processor,\n",
    "               output_text_processor, \n",
    "               use_tf_function=True):\n",
    "    super().__init__()\n",
    "    encoder = Encoder(input_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "    decoder = Decoder(output_text_processor.vocabulary_size(), embedding_dim, units)\n",
    "\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_text_processor = input_text_processor\n",
    "    self.output_text_processor = output_text_processor\n",
    "    self.use_tf_function = use_tf_function\n",
    "\n",
    "  def train_step(self, inputs):\n",
    "    if self.use_tf_function:\n",
    "      return self._tf_train_step(inputs)\n",
    "    else:\n",
    "      return self._train_step(inputs)\n",
    "      \n",
    "  def _preprocess(self, input_text, target_text):\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    target_tokens = self.output_text_processor(target_text)\n",
    "    input_mask = input_tokens != 0\n",
    "    target_mask = target_tokens != 0\n",
    "    return input_tokens, input_mask, target_tokens, target_mask\n",
    "\n",
    "  def _train_step(self, inputs):\n",
    "    input_text, target_text = inputs  \n",
    "    (input_tokens, input_mask, target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "    max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      enc_output, enc_state = self.encoder(input_tokens)\n",
    "      dec_state = enc_state\n",
    "      loss = tf.constant(0.0)\n",
    "\n",
    "      for t in tf.range(max_target_length-1):\n",
    "        new_tokens = target_tokens[:, t:t+2]\n",
    "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                              enc_output, dec_state)\n",
    "        loss = loss + step_loss\n",
    "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "    variables = self.trainable_variables \n",
    "    gradients = tape.gradient(average_loss, variables)\n",
    "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return {'batch_loss': average_loss}\n",
    "  \n",
    "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "    decoder_input = DecoderInput(new_tokens=input_token, enc_output=enc_output, mask=input_mask)\n",
    "\n",
    "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "    y = target_token\n",
    "    y_pred = dec_result.logits\n",
    "    step_loss = self.loss(y, y_pred)\n",
    "\n",
    "    return step_loss, dec_state\n",
    "    \n",
    "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]), tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "  def _tf_train_step(self, inputs):\n",
    "      return self._train_step(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainChatbot._preprocess = _preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainChatbot._train_step = _train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainChatbot._loop_step = _loop_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = TrainChatbot(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=False)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "chatbot.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainChatbot._tf_train_step = _tf_train_step\n",
    "chatbot.use_tf_function = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chatbot = TrainChatbot(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor)\n",
    "\n",
    "\n",
    "train_chatbot.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, key):\n",
    "    self.key = key\n",
    "    self.logs = []\n",
    "\n",
    "  def on_train_batch_end(self, n, logs):\n",
    "    self.logs.append(logs[self.key])\n",
    "\n",
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "46/46 [==============================] - 144s 3s/step - batch_loss: 4.1654\n",
      "Epoch 2/2\n",
      "46/46 [==============================] - 146s 3s/step - batch_loss: 3.8648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23682edfee0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_chatbot.fit(train_data, epochs=2, callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Chatbot(tf.Module):\n",
    "    def __init__(self, encoder, decoder, input_text_processor, output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "\n",
    "        self.output_token_string_from_index = (tf.keras.layers.StringLookup(vocabulary=output_text_processor.get_vocabulary(), mask_token='', invert=True))\n",
    "\n",
    "        index_from_string = tf.keras.layers.StringLookup(vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "\n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))\n",
    "\n",
    "    def tokens_to_text(self, result_tokens):\n",
    "        result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "        result_text = tf.strings.reduce_join(result_text_tokens, axis=1, separator=' ')\n",
    "        result_text = tf.strings.strip(result_text)\n",
    "        return result_text\n",
    "\n",
    "    def sample(self, logits, temperature):\n",
    "        token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "        logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            new_tokens = tf.argmax(logits, axis=-1)\n",
    "        else: \n",
    "            logits = tf.squeeze(logits, axis=1)\n",
    "            new_tokens = tf.random.categorical(logits/temperature, num_samples=1)\n",
    "        return new_tokens\n",
    "\n",
    "    def chatbot_unrolled(self, input_text, *, max_length=50, return_attention=True, temperature=1.0):\n",
    "        batch_size = tf.shape(input_text)[0]\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "        dec_state = enc_state\n",
    "        new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "        result_tokens = []\n",
    "        attention = []\n",
    "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            dec_input = DecoderInput(new_tokens=new_tokens, enc_output=enc_output, mask=(input_tokens!=0))\n",
    "            dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "            attention.append(dec_result.attention_weights)\n",
    "            new_tokens = self.sample(dec_result.logits, temperature)\n",
    "            done = done | (new_tokens == self.end_token)\n",
    "            new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "            result_tokens.append(new_tokens)\n",
    "\n",
    "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "                break\n",
    "\n",
    "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "        result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "        if return_attention:\n",
    "            attention_stack = tf.concat(attention, axis=1)\n",
    "            return {'text': result_text, 'attention': attention_stack}\n",
    "        else:\n",
    "            return {'text': result_text}\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "    def tf_translate(self, input_text):\n",
    "        return self.translate(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "input_text = tf.constant([\n",
    "    'hi, how are you doing?', # \"It's really cold here.\"\n",
    "    \"i'm fine. how about yourself?\"\n",
    "])\n",
    "\n",
    "result = chatbot.translate(\n",
    "    input_text = input_text)\n",
    "\n",
    "print(result['text'][0].numpy().decode())\n",
    "print(result['text'][1].numpy().decode())\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c15d18a1b1a68c3d158c4be9ef425733a9df674fbdaa7d86764bd646a5999f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
